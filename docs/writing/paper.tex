% !TeX root = paper.tex
\documentclass[a4paper,12pt]{article}

% Packages
\usepackage[utf8]{inputenc}     % For UTF-8 encoding
\usepackage{amsmath, amssymb}   % For math symbols
\usepackage{graphicx}           % For including graphics
\usepackage{geometry}           % For setting page margins
\usepackage[numbers]{natbib}             % For citations
\usepackage{hyperref}           % For clickable links in references
\usepackage{subcaption}         % For side by side subfigures
% Page layout
\geometry{margin=1in}           % Set margins to 1 inch

% Title section
\title{Tina Paper In Development}
\author{Your Name}
\date{\today}




\begin{document}

\maketitle

\begin{abstract}
    This is the abstract of your paper. Briefly describe the purpose of the research, the main results, and the conclusions.
\end{abstract}

\section{Introduction}
The study of tree rings has proven useful across multiple fields, proving to be a reliable subject for reconstructing past climates of regional and local environments, as well as a mechanism to understand tree growth response \citep{fritts_dendroclimatology_1971} \citep{williams_using_2010} \citep{guibal_dendrochronology_2021} \citep{sheppard_dendroclimatology_2010}.
To obtain these data from the tree rings, it is necessary to measure tree rings from a tree cookie or core. The first high precision tool for this purpose was a stage micrometer, involving a trained technician to incrementally shift a tree core under the objective of a microscope - informing a computer when a new ring is encountered \citep{robinson_microcomputer_nodate}.
While this method has very high precision, the data is only as accurate as the experience and knowledge of the technician at the time of recording \citep{levanic_atrics_2007}.
The desire to remove repetition of errors in sampling and sampling bias across to individual technicians led researchers to an alternative - image analysis. 

The first step in measuring tree ring width from images requires the digitization of the sample from one of two major methods.  
The original technique was as a flatbed scanner which can digitize the entire sample at once \citep{guay_new_1992}. 
With a top of the line scanner, like the Epson Perfection V850 Pro, it's possible to scan at maximum resolutions of 4800 dpi and scan an area of up to 8.5" x 11.7".
Analysis which relies on higher resolution larger samples require a different digitization approach. 

The second digitization model was introduced with ATRICS \citep{levanic_atrics_2007}. 
Rather than scanning a whole sample at once, a high resolution camera takes multiple images across the surface of the sample and uses image stitching software techniques to combine them into one ultra high resolution image \citep{muhlich_stitching_2022}.
This methodology is often seen in other fields such as mineralogy and cellular biology \citep{ro_image_2021,mohammadi_fast_2024}. 
This method requires either the camera objective to move relative to the sample, or the sample move underneath a stationary camera. 
For ATRICS and a more modern do-it-yourself alternative, CaptuRING, the sample is moved relative to the camera \citep{garcia-hidalgo_capturing_2022}. 
Gigapixel takes a different approach by moving the camera relative to the sample, allowing for multiple samples to be recorded in sequence. 
While these machines can all digitize cores, none have been shown to digitize cookies.

Tina was made to combine the defining features of the previously mentioned machines into one while making the code open-source and chassis open-hardware device. 
We designed Tina to digitize both cookies and cores, extend the maximum sample length, perform image stitching without operator intervention, while minimizing cost. 
Successful functionality of this system is dependent on the major steps in tree cookie and core digitization to be automated. 
The only specialized piece of equipment needed to build Tina is a 3D printer, but the parts can be readily ordered through 3D print shops if preferred.
Excluding 3D printed parts, the total cost of the machine is approximately \$2,200 USD compared to the \$70,000 USD of the Gigapixel \citep{griffin_gigapixel_2021}.
The total cost of the machine is almost comparable in price or less to many professional camera and macro lens combinations.
Additional savings can also be made when factoring in the cost of a professional stitching software license such as PTGui. 

\section{System Overview} % this is so dry fix later

Tina can be thought of as a combination of multiple subsystems: the camera, computer, and gantry machine. The first system is the camera and computer. 
A 12MP Raspberry Pi HQ Camera equipped with a SEEED studio microscope lens is connected to an NVIDIA Jetson Orin Nano (Jetson) through a CSI cable. 
The Jetson is a powerful SoC edge computer which drives a computer monitor for a GUI, sends commands to the motor controller to move the machine, runs image processing calculations for automatic control, and performs calculations to stitch individual images into one mosaic. 
The gantry machine is built using the extruded aluminum and premade adapters from the OpenBuilds ACRO kit. 
Movement of the machine is originates from NEMA 17 stepper motors and is transferred to
linear motion in the X and Y direction using timing belts and timing pulleys. 
Motion in the Z-direction is controlled by a NEMA 17 lead screw based linear actuator also supplied by OpenBuilds.
Custom 3D models were designed in SolidWorks to create adapters to mount the camera to the Z axis as well as mounting the Z axis to the XY plate of the ACRO. 
The OpenBuilds X32 Motor Controller running GRBL firmware converts G-Code commands received via serial from the Jetson. GRBL then converts them to electrical signals to spin the lead screw and timing belts. 

Interacting with the Jetson is similar to working with any Ubuntu based Linux distribution. 

\section{Sample Digitization} % Methods
The subsystems can be best understood by following the path from sample setup through the computing process to create the stitched image. 

To achieve this with a fixed focus camera, the samples must be nearly orthogonal to the camera lens. Misalignments between the sample and lens can be corrected by using the 3D printed sample levelling table we designed.
Once the sample is level, the operator interacts with the machine through the GUI to navigate the camera to the center of the sample, and initializes focus to be sharp. The height and width dimensions are then entered in the GUI
to be saved along with the center XYZ coordinate of the sample. This procedure can be repeated to create a queue of samples to digitize.

Sample height, width, identifiers, and centering an in focus sample is sufficient information to digitize.

\subsection{Image Capturing}
Once prompted the system begins to exhaustively traverse the surface area of the sample. 
The goal of this traversal is to obtain in-focus images that have a region of overlap with all its neighbors - the basis of image stitching. 
Digitizing cores can often be done without needing to traverse in both the X and Y axis as the field of view in an image can capture the core's entire axial diameter in its field of view when moving along the core's length. % confusing wording fix later
Spanning the surface area of a cookie is more complicated as it needs move in both the X and Y axis (see Figure).

By centering the design around a fixed focus camera and lens, it is necessary to implement some sense of automatic control to focus the images. 
In this system, the only way to control focus is by changing the lens's distance to the sample. 
And with the microscope lens, the depth of field of the image is so sensitive that a perturbation of less half of a millimeter can move an entire image out of focus.
Solving this in a time efficient manner involves two stages. 
The first stage takes advantage of the requirement to navigate and focus the camera to the center of the sample.
This allows the initial $Z_0$ value from $(X_0, Y_0, Z_0)$ to be a fantastic initial guess as to what value would have an in focus image across the entire surface. 
Sample surface misalignment and a non flat sample (bumpy) surfaces require more than one image to be taken at each image $(X, Y)$. 
From the first $(X_0, Y_0)$ coordinate captured, 11 images are taken at different $Z$ values in $\boldsymbol{Z_{\text{focus}}}$. 
So long as $Z_{sample}$ is within this set, an in focus image exists.

\[
Z_{sample} \in
\boldsymbol{Z_{\text{focus}}} = 
Z_0 + 
\begin{bmatrix}
-0.5 \\
-0.4 \\
\vdots \\
0.4 \\
0.5 \\
\end{bmatrix}
\exists 
Z_{focused}
\] % unsure if this makes sense

Beginning at 0.5 mm above $Z_0$ and the last image finishing at 0.5 mm below $Z_0$.
To reduce motion blur in the images, this $\boldsymbol{Z_{\text{focus}}}$ is traversed at constant velocity and images are captured without stopping. 
The 11 images then have their normalized variance calculated in a separate thread to measure the image sharpness \citep{sampat_extensive_2014}.
The image with the maximum score is saved while the rest are deleted from storage.


\[
    i_{\max} = \arg\max_{i} \boldsymbol{Z_{\text{focus}}}
\]
    
This focusing procedure works well alone when the sample alignment has a difference in height no greater than 1 mm. 
Greater alignment error can be managed by having the control over adjusting $Z_{0,k}$ for $(X_k,Y_k)$.
The likelihood of an adjacent $(X_{k+1}, Y_{k+1})$ containing an in focus image is highest when the current $i_{{\max,k}}$ is at the middle index of $\boldsymbol{Z_{focus}}$. 
A PID control algorithm with a process variable of $i_{max}$ and control variable of $Z_{0,k}$ allows a negative feedback loop to improve focusing across the entire sample \citep{odwyer_summary_2000}. 

After the capturing process, the images are stitched and are stored on the device along with metadata of relevant machine
settings and sample information. 

The frame, camera, computer, software, and sample leveling tables, work cohesively to address the nuances of digitizing both cookies and cores. 
Each of these subsystems will be reviewed along with the design considerations involved to make a complete machine. 
Considering Tina is an imaging machine, we will start off by looking at the camera and move outward.


\subsection{Camera}
In the Gigapixel and CaptuRING, a professional handheld camera was used to capture images. 
While this benefits from a wide variety of lens and camera choices - and principally autofocus - they come at a cost. 
Beyond its literal price tag, a professional camera also carries a significant weight, demanding a more robust and therefore more costly machine frame. 
These cameras are also optimized for handheld use, not for integration into computer driven machines. 
This results in clunky access to images from software. 
Designing around a machine vision camera such as the 12MP Raspberry Pi HQ Camera drastically reduced the camera cost while improving the access to the data stream coming from the camera sensor.
Pairing the Raspberry Pi HQ Camera with a high magnification C-Mount lens allows for images with very high resolution in dots-per-inch (DPI) to be captured. (THIS IS AN ESTIMATE, GET THE REAL VALUES FROM TINA) If the field of view of the image

\subsubsection{Focusing}
Normalized variance process goes here.

Achieving a focused image across the entire surface area of the sample is the basis for an accurate stitch and effective analysis. 
By saving costs on a fixed focus camera and lens, it was necessary to implement a method to focus the images. 
Focus is solely a function of distance to the subject in the regime. 
With this high magnification lens, having an image go from in-focus to out-of-focus was sensitive to sub millimeter variation in distance to the lens. 
This requires cores and especially cookies to be as close to orthogonally aligned to the lens as possible. 
A 3D printed levelling table and bullseye level were used to correct plane misalignment of up to 10 degrees in cookies (CALCULATE THIS?). 
Slight sample misalignments are corrected within a closed loop automatic control PID algorithm. 


%% Previous Methods is commented out so I can reorder things 
% \subsection{Hardware Design}

% \subsubsection{Frame}
% The cartesian gantry machine was purchased from an open source parts supplier, OpenBuilds. They sell various sets of gantry robots, motor controllers, and power supplies
% which can be conveniently purchased from one supplier. All metal components are cut and sized, and have very detailed assembly videos, an active community forum,
% and customer support to fall back on. The OpenBuilds ACRO system circumvents the need for most power tools, machining, electrical connectors, and technical knowhow that would otherwise be necessary to build a robust robot.
% All other parts were designed using SolidWorks and printed on an FDM 3D printer using ABS material. Mechanical assembly can be completed entirely with common hand tools and metric fasteners. 

% \subsubsection{Computer}
% The computer which runs the graphical user interface (GUI), gantry control, camera streaming, and image stitching is the NVIDIA Jetson Orin Nano Developer Kit (Orin Nano). Using a computer
% which is compatible with CSI cameras such as the Raspberry Pi HQ Camera was essential to maintain cost effectiveness. The Orin Nano, like others in the Jetson line of products, has hardware
% accelerators for common GPU tasks such as image streaming, which is vital to retain CPU power for GUI tasks and robot control. 

% \subsubsection{Camera}
% Unlike the Gigapixel or CaptuRING, the camera is a machine vision camera rather than a handheld professional camera. Choosing the Raspberry Pi HQ Camera provided many lens options, significant
% forum support, and includes an Obsolescence Statement that the camera will be in production until January 2030. Combined with a 180mm C-mount microscope lens, the images can reach above 20,000 dots per inch (DPI).

% \subsection{Software Design}
% All software was written in Python VERSION and a variety of packages for the GUI, image stitching, and calculations (Should I include all the packages in writing?). Care was taken to segment the
% software design into abstracted object oriented code. 

% \subsubsection{Graphical User Interface}
% The GUI to control the machine launches from a Python script. Included are a viewer to see the live stream from the camera, buttons to jog the machine throughout the XYZ coordinate system,
% setting the size of samples, and running the process to capture and stitch images. The system was designed to allow for multiple samples to be prepared in a queue for bulk digitization. 

% \subsubsection{Sample Digitization}
% The digitization of a sample can be reduced to a few processes - capturing a grid of overlapping images, image focusing, and image stitching. 

% Inside the GUI, the program is populated with the height and width of the sample as well as the height and width of the image. The user then navigates to the center of the sample and can choose 
% to begin imaging the sample or add the sample to a queue for bulk digitization. This provides the machine enough data to calculate the necessary rows and columns for traversing the entire surface
% area of the sample while maintaining a level of overlap between adjacent images. Image overlap is what allows for images to be stitched together as there is nonzero error when the robot translates 
% from one coordinate to another. 

% To stitch adjacent images, feature based image stitching from a Python package Stitch2D was used. The underlying functions were based on the processing libraries
% OpenCV and NumPy. Multiple other image stitching packages were tested but none were able to successfully piece together our samples. (***Sounds kinda bad to not have data on a comparison as well as not having a metric to see how 'well' an image is stitched... but this is non trivial. Can we get away with it?***)

% One major difficulty with using the Raspberry Pi HQ Camera is the lack of a lens with auto-focus. An in focus image in this regime is dependent on the focal length of the lens set by the user and the
% distance from the sample. The microscope lens has a very small depth of field and goes from in-focus to out-of-focus in less than a 1mm step in the distance to the sample. Two cooperative solutions were implemented 
% to achieve a focused image for the entire surface area of the sample - taking images from multiple distances from the sample and automatic control. 

% \subsubsection{Image Stack Focusing}
% When the sample is placed on the table to be digitized, it is assumed that there will, at best, very slight variations in height across the surface area of the sample. A levelling table 
% assists in reducing this error although it will never be nonzero. At each location (X, Y) on the grid of overlapping images, 11 images are captured in equally distanced Z steps across a 1mm range. 
% Each of these images have a normalized variance score calculated - the maximum value represents the most in focus image \citep{sampat_extensive_2014}. In practice, stopping and starting the motor causes 
% enough vibration to require upwards to two seconds of pause time before the motion blur stops to take each photo. Instead of stopping and starting each image, an additional distance is added to the 
% Z-motion to allow the stepper to accelerate and reach constant velocity. At this speed, images are then taken with constant time steps instead of constant distance steps across the 1mm range. This eliminates
% the motion blur due to acceleration and dramatically reduces imaging time. 

% $$\frac{1}{MN\mu} \sum\limits_{x=0}^{M-1} \sum\limits_{y=0}^{N-1}(f(x,y) - \mu)^2$$

% \subsubsection{Automatic Control}
% While the image stack provides a basis to obtain an in focus image, it is statically limited to the 1mm range in the Z axis. Instead of spending significant time guaranteeing that the sample
% is within the the Z axis range across the entire surface area, a PID control algorithm takes the wheel \citep{odwyer_summary_2000}. The information previously calculated in the image stack is sufficient 
% to inform the PID controller of how to adjust the initial the Z axis for the next image stack. The controller is constantly trying to make the most in focus image be at the center of the image stack,
% index value 5. This allows for the most flexibility moving to the next location of the image grid. 

% \begin{figure}
%     \centering
%     \begin{subfigure}{.5\textwidth}
%       \centering
%       \includegraphics[height=0.75\linewidth]{../diagrams/sample_setup_ideal.png}
%       \caption{Ideal sample leveling}
%       \label{fig:sub1}
%     \end{subfigure}%
%     \begin{subfigure}{.5\textwidth}
%       \centering
%       \includegraphics[height=0.75\linewidth]{../diagrams/sample_setup_realistic.png}
%       \caption{Realistic sample leveling}
%       \label{fig:sub2}
%     \end{subfigure}
%     \caption{Side view of the camera and sample on top of a leveling table. The ideal sample leveling shows a uniform distance d at all (x,y) coordinates on the sample. This is impossible to achieve in reality, the true sample leveling has a non uniform distance at unique (x,y) coordinates.}
%     \label{fig:test}
% \end{figure}


\section{Results}
\subsection{Scans of Cookies and Cores}
The microscope lens greatly improved the maximum resolution of the digitization. Resolutions of up to 13,400 DPI were achieved, a large improvement when compared to 
both high resolution flatbed scanners (Epson® Perfection v750 PRO) and CaptuRING. 

\subsection{Functional Limits}
With such high resolution, multiple logistics concerns arise. First is the file size. With such a high resolution, the size of the images can become extremely large. A cookie
5 inches in diameter would result in an image with 67,000 pixels squared. This easily exceeds the 2.5 GB maximum file size in a TIFF - most software for viewing the file will also 
be incompatible with any file this big as well. 


\section{Discussion}
Discuss the implications of your results here.

\subsection{Strengths and Opportunities}

\subsection{Opportunities for Improvement}

\section{Conclusion}
Summarize your key findings here.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
