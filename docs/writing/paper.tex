% !TeX root = paper.tex
\documentclass[a4paper,12pt]{article}

% Packages
\usepackage[utf8]{inputenc}     % For UTF-8 encoding
\usepackage{amsmath, amssymb}   % For math symbols
\usepackage{graphicx}           % For including graphics
\usepackage{geometry}           % For setting page margins
\usepackage{natbib}             % For citations
\usepackage{hyperref}           % For clickable links in references

% Page layout
\geometry{margin=1in}           % Set margins to 1 inch

% Title section
\title{Tina Paper In Development}
\author{Your Name}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    This is the abstract of your paper. Briefly describe the purpose of the research, the main results, and the conclusions.
\end{abstract}

\section{Introduction}
This is the introduction section where you introduce the topic and cite some papers \citep{mohammadi_comparative_2024}.

\section{Methods}
Describe your methods here. You can use equations like this:

\subsection{Hardware Design}

\subsubsection{Mechanical Structure}
The cartesian robot was purchased from an open source parts supplier, OpenBuilds. They sell various sets of gantry robots, motor controllers, and power supplies
which can be conveniently purchased from one supplier. All metal components are cut and sized, and have very detailed assembly videos, an active community forum,
and customer support to fall back on. The OpenBuilds ACRO system circumvents the need for most power tools, machining, electrical connectors, and technical knowhow that would otherwise be necessary to build a robust robot.
All other parts were designed using SolidWorks and printed on an FDM 3D printer using ABS material. Mechanical assembly can be completed entirely with common hand tools and metric fasteners. 

\subsubsection{Computer}
The computer which runs the graphical user interface (GUI), gantry control, camera streaming, and image stitching is the NVIDIA Jetson Orin Nano Developer Kit (Orin Nano). Using a computer
which is compatible with CSI cameras such as the Raspberry Pi HQ Camera was essential to maintain cost effectiveness. The Orin Nano, like others in the Jetson line of products, has hardware
accelerators for common GPU tasks such as image streaming, which is vital to retain CPU power for GUI tasks and robot control. 

\subsubsection{Camera}
Unlike the Gigapixel or CaptuRING, the camera is a machine vision camera rather than a handheld professional camera. Choosing the Raspberry Pi HQ Camera provided many lens options, significant
forum support, and includes an Obsolescence Statement that the camera will be in production until January 2030. Combined with a 180mm C-mount microscope lens, the images can reach above 20,000 dots per inch (DPI).

\subsection{Software Design}
All software was written in Python VERSION and a variety of packages for the GUI, image stitching, and calculations (Should I include all the packages in writing?). Care was taken to segment the
software design into abstracted object oriented code. 

\subsubsection{Graphical User Interface}
The GUI to control the machine launches from a Python script. Included are a viewer to see the live stream from the camera, buttons to jog the machine throughout the XYZ coordinate system,
setting the size of samples, and running the process to capture and stitch images. The system was designed to allow for multiple samples to be prepared in a queue for bulk digitization. 

\subsubsection{Sample Digitization}
The digitization of a sample can be reduced to a few processes - capturing a grid of overlapping images, image focusing, and image stitching. 

Inside the GUI, the program is populated with the height and width of the sample as well as the height and width of the image. The user then navigates to the center of the sample and can choose 
to begin imaging the sample or add the sample to a queue for bulk digitization. This provides the machine enough data to calculate the necessary rows and columns for traversing the entire surface
area of the sample while maintaining a level of overlap between adjacent images. Image overlap is what allows for images to be stitched together as there is nonzero error when the robot translates 
from one coordinate to another. 

To stitch adjacent images, feature based image stitching from a Python package Stitch2D was used. The underlying functions were based on the processing libraries
OpenCV and NumPy. Multiple other image stitching packages were tested but none were able to successfully piece together our samples. (***Sounds kinda bad to not have data on a comparison as well as not having a metric to see how 'well' an image is stitched... but this is non trivial. Can we get away with it?***)

One major difficulty with using the Raspberry Pi HQ Camera is the lack of a lens with auto-focus. An in focus image in this regime is dependent on the focal length of the lens set by the user and the
distance from the sample. The microscope lens has a very small depth of field and goes from in-focus to out-of-focus in less than a 1mm step in the distance to the sample. Two cooperative solutions were implemented 
to achieve a focused image for the entire surface area of the sample - taking images from multiple distances from the sample and automatic control. 

\subsubsection{Image Stack Focusing}
When the sample is placed on the table to be digitized, it is assumed that there will, at best, very slight variations in height across the surface area of the sample. A levelling table 
assists in reducing this error although it will never be nonzero. At each location (X, Y) on the grid of overlapping images, 11 images are captured in equally distanced Z steps across a 1mm range. 
Each of these images have a normalized variance score calculated - the maximum value represents the most in focus image \citep{sampat_extensive_2014}. In practice, stopping and starting the motor causes 
enough vibration to require upwards to two seconds of pause time before the motion blur stops to take each photo. Instead of stopping and starting each image, an additional distance is added to the 
Z-motion to allow the stepper to accelerate and reach constant velocity. At this speed, images are then taken with constant time steps instead of constant distance steps across the 1mm range. This eliminates
the motion blur due to acceleration and dramatically reduces imaging time. 

$$\frac{1}{MN\mu} \sum\limits_{x=0}^{M-1} \sum\limits_{y=0}^{N-1}(f(x,y) - \mu)^2$$

\subsubsection{Automatic Control}
While the image stack provides a basis to obtain an in focus image, it is statically limited to the 1mm range in Z-values. Instead of spending significant time guaranteeing that the sample
is within the Z-value range across the entire surface area, a PID control algorithm takes the wheel \citep{odwyer_summary_2000}. The information previously calculated in the image stack is sufficient 
to inform the PID controller of how to adjust the initial Z-value for the next image stack. The controller is constantly trying to make the most in focus image be at the center of the image stack,
index value 5. This allows for the most flexibility moving to the next location of the image grid. 


\section{Results}
Present your results here.

\subsection{Scans of Cookies and Cores}

\subsection{Functional Limits}

\section{Discussion}
Discuss the implications of your results here.

\subsection{Strengths and Opportunities}

\subsection{Opportunities for Improvement}

\section{Conclusion}
Summarize your key findings here.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
